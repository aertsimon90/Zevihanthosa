# **Zevihanthosa - Advanced Artificial Intelligence Framework**  
![ZevihaNut Logo](ZevihaNut.png)

## ZevihaNut/2.9 Model (Latest Release - January 2026)
*A lightweight, pure-Python hybrid AI framework combining parametric neurons, non-parametric memory, safe symbolic regression, massively scaled localized ensembles, and Deep Multi-Layer Networks â€” now with significantly improved numerical stability, memory reset control, and quantization-aware behavior.*

> ðŸ‘‰ **New Reader?**  
> Read the manifesto: **[WHATADIFF.md â€” What a Difference?](WHATADIFF.md)**

### Overview
**Zevihanthosa 2.9** brings important stability and control improvements while preserving the framework's core philosophy: extreme minimalism, full online/incremental learning, complete serialization, and hybrid parametric + non-parametric intelligence.

**Main novelties in 2.9:**

- Every single cell type now has three powerful new stabilization & control methods:
  - `.forget()` â€” completely resets all momentum accumulators (mw, mb, etc.)
  - `.rmtolerance(sensitivity=128)` â€” rounds values that are very close to integers back to exact integers (helps escape floating-point drift and creates more "crisp" decision boundaries)
  - `.quantization(sensitivity=128..512)` â€” aggressive value quantization for better numerical stability and potential memory/performance gains in long-running systems
- These three methods are implemented consistently across **all** cell types (Cell, MultiInputCell, MultiCell, CellForest, CellNetwork, FuncCell, etc.)
- Improved `squeezeforclust` & cluster-based routing stability
- More predictable behavior in very long training runs and deep forest/network structures

### Key Features

- Pure Python & Minimal Dependencies (`random`, `math`, `json`, `ast`)
- Fully Online/Incremental Learning â€“ adapts instantly per sample
- Deep Hierarchical Backpropagation through arbitrary layer counts
- Momentum Optimization/Exchange (default 0.9)
- Automatic Weight/Bias Clamping
- Flexible Activations: sigmoid, softsign, tanh, Swish, Positive/Negative Linear, custom zevian
- **Complete Model Persistence** â€” save/load/copy any component to/from JSON
- **Every cell now includes stabilization toolkit:**
  - `.forget()` â€” reset all momentum buffers
  - `.rmtolerance(sensitivity)` â€” smart near-integer snapping for stability
  - `.quantization(sensitivity)` â€” controlled value discretization
- 13 Learning Paradigms (all enhanced with new stabilization methods):

  1. Classic momentum neuron â†’ `Cell`
  2. Additive linear unit â†’ `LinearSumCell`
  3. Multiplicative linear unit â†’ `LinearMulCell`
  4. Instance-based memory regressor â†’ `DataCell`
  5. Multi-input fusion neuron â†’ `MultiInputCell`
  6. Parallel multi-output neuron â†’ `MultiOutputCell`
  7. Full dense layer (multi-in â†’ multi-out) â†’ `MultiCell`
  8. Safe symbolic regression (AST-evaluated) â†’ `FuncCell`
  9. Minimal perceptron (no momentum) â†’ `NanoCell`
  10. Localized single-input ensemble â†’ `CellForest`
  11. Localized full dense ensemble â†’ `MultiCellForest`
  12. Deep Multi-Layer Backpropagation Network â†’ `CellNetwork`
  13. Localized ensemble of deep multi-layer networks â†’ `CellNetworkForest`

- High Interpretability â€” readable symbolic formulas + persistent JSON states
- Aggressive Quantization, Tolerance Snapping & Compression Tools
- Center-aware localization + anchor-based ensemble blending

### Core Components
#### 1. `Cell` â€” Classic Momentum-Optimized Neuron
<img src="images/image1.png" width="800" height="400">

#### 2. `LinearSumCell` â€” `output = activation(input + bias)`
<img src="images/image2.png" width="800" height="400">

#### 3. `LinearMulCell` â€” `output = activation(input * weight)`
<img src="images/image3.png" width="800" height="400">

#### 4. `DataCell` â€” Distance-weighted memory-based prediction
<img src="images/image4.png" width="800" height="400">

#### 5. `MultiInputCell` â€” Multiple inputs â†’ single output
<img src="images/image5.png" width="800" height="400">

#### 6. `MultiOutputCell` â€” Single input â†’ multiple outputs
<img src="images/image6.png" width="800" height="400">

#### 7. `MultiCell` â€” Full arbitrary dense layer
<img src="images/image7.png" width="800" height="400">

#### 8. `FuncCell` â€” Safe symbolic regression (bounded arithmetic expressions)
<img src="images/image8.png" width="800" height="400">

#### 9. `NanoCell` â€” Lightweight perceptron (no momentum)
<img src="images/image9.png" width="800" height="400">

#### 10. `CellForest` â€” **Localized** ensemble of `Cell`s
<img src="images/image10.png" width="800" height="400">

#### 11. `MultiCellForest` â€” **Localized** ensemble of `MultiCell`s
<img src="images/image11.png" width="800" height="400">

#### 12. `CellNetwork` â€” Multi-layer dense network with automated backpropagation and per-layer activations
<img src="images/image12.png" width="800" height="400">

#### 13. `CellNetworkForest` â€” Localized ensemble of `CellNetwork`s
<img src="images/image13.png" width="800" height="400">

### New in ZevihaNut/2.9

* **Refined Multi-Dimensional Compression:** The `squeezeforclust` mechanism provides a balanced method for encoding multi-feature inputs into compact coordinates for forest routing.
* **Controllable Expert Scaling:** Rather than fixed exponential growth, the introduction of the `forest_depth` parameter allows for a tunable number of experts in `MultiCellForest` and `CellNetworkForest`.
* **Anchor-Based Ensemble Blending:** The most relevant expert (the "anchor") now exerts a dominant influence on the final output, ensuring that the highest-scoring unit leads the prediction.
* **Center-Aware Localization:** Relevance computation utilizes `(i + 0.5)` centering to create smoother transitions and gradients between neighboring experts in the forest.
* **Cluster-Aware Quantization:** New `clustmin` and `clustmax` functions facilitate specialized clamping, ensuring quantization is stable and compression-aware.
* **Optimized Forest Routing:** Multi-dimensional forests now leverage `squeezeforclust` to handle structured multi-feature inputs more effectively than simple averaging.
* **Efficiency via Top-K Synergy:** By combining limited `top_k` selection with strong anchor weighting, the system improves both computational efficiency and output quality.
* **Full Stabilization Toolkit Across All Cells:**  
  Every single cell type (from basic `Cell` to complex `MultiCellForest` and `CellNetwork`) now includes three powerful new control & stability methods:
  - `.forget()` â†’ Instantly resets all momentum accumulators (mw, mb, etc.) â€” perfect for starting fresh without reinitializing the whole model
  - `.rmtolerance(sensitivity=128)` â†’ Intelligently snaps near-integer values to exact integers â€” dramatically reduces floating-point drift accumulation over very long training sessions
  - `.quantization(sensitivity=128..1024)` â†’ Applies controlled discretization of weights, biases, and learning rates â€” improves long-term numerical stability and can reduce memory footprint in extreme cases

> [!TIP] 
> **Resolved Limitations in 2.9 Previous bottlenecks have been addressed to ensure a more stable development experience:**
> 
> Scalability: The transition to forest_depth has eliminated the memory issues associated with raw exponential growth, allowing for deeper architectures without crashing.
> 
> Performance: Training time and routing quality have been optimized through the squeezeforclust mechanism and localized targeting.
> 
> Simplified Tuning: The complex distance_sharpness parameter has been removed in favor of more intuitive top_k and clusters settings.
> 
> High-Dimensional Stability: Multi-dimensional routing is now more robust, allowing for better handling of complex input features without the need for external dimensionality reduction.

Thank you for following Zevihanthosa development â€” 2.9 represents a bold step toward expert specialization.

---

### Installation

```bash
git clone https://github.com/aertsimon90/Zevihanthosa.git
cd Zevihanthosa
# Import zevihanthosa.py directly into your project

```

No pip install required.

### Usage Examples

```python
from zevihanthosa import *
import random
import math

random.seed(42) # stability

# 1. Cell â€“ learning a threshold
cell = Cell(learning=0.15)
for _ in range(12000):
    x = random.random()
    target = 1.0 if x > 0.7 else 0.0
    cell.process(x, target)
print("Cell at 0.6 â†’", cell.process(0.6, train=False))
print("Cell at 0.8 â†’", cell.process(0.8, train=False))

# 2. DataCell â€“ adaptive averaging / denoising
dc = DataCell(maxdatac=100)
for i in range(300):
    noisy = 0.5 + random.gauss(0, 0.12)
    dc.process(noisy, target=0.5)
print("Current smooth estimate:", dc.process(0.7, train=False))

# 3. MultiInputCell â€“ learning harmonic mean approximation
mi = MultiInputCell(wcount=2, learning=0.08)
for _ in range(30000):
    a = random.randint(1, 10)
    b = random.randint(1, 10)
    mi.process([1/a, 1/b], target=1/(a + b))  # 1/x normalization for 0-1 range
print("MultiInputCell approx 1/(7+2) from [1/7, 1/2]:", 1/mi.process([1/7, 1/2], train=False))

# 4. MultiOutputCell â€“ predicting xÂ² and âˆšx from x
mo = MultiOutputCell(pcount=2, learning=0.1)
for _ in range(20000):
    x = random.uniform(0, 1)
    target1 = x ** 2
    target2 = math.sqrt(x)
    mo.process(x, target=[target1, target2])
print("x = 0.0 â†’ (xÂ², âˆšx):", mo.process(0.0, train=False))
print("x = 0.25 â†’ (xÂ², âˆšx):", mo.process(0.25, train=False))
print("x = 0.5 â†’ (xÂ², âˆšx):", mo.process(0.5, train=False))
print("x = 1.0 â†’ (xÂ², âˆšx):", mo.process(1.0, train=False))

# 5. FuncCell â€“ discovering square root
fc = FuncCell(traindepth=2)
for x in [0,1,4,9,16]:
    y = math.sqrt(x)
    fc.process(x, target=y)
print("Discovered function:", fc.func)
print("Test on x=3 â†’", fc.process(3, train=False))

# 6. MultiCell â€“ dividing the sum of two numbers by three (equal share to each output) test (new in 2.3)
mc = MultiCell(icount=2, ocount=3, learning=0.15)
for _ in range(30000):
    a = random.uniform(0.0, 1.0)
    b = random.uniform(0.0, 1.0)
    total = a + b
    third = total / 3.0
    mc.process([a, b], target=[third, third, third])
print("Input [0.0, 0.0] â†’", mc.process([0.0, 0.0], train=False))      # expected: [0.0, 0.0, 0.0]
print("Input [0.3, 0.3] â†’", mc.process([0.3, 0.3], train=False))      # expected: [0.2, 0.2, 0.2]
print("Input [0.5, 0.6] â†’", mc.process([0.5, 0.6], train=False))      # expected: â‰ˆ[0.3667, 0.3667, 0.3667]
print("Input [1.0, 1.0] â†’", mc.process([1.0, 1.0], train=False))      # expected: â‰ˆ[0.6667, 0.6667, 0.6667]
print("Input [0.2, 0.8] â†’", mc.process([0.2, 0.8], train=False))      # expected: â‰ˆ[0.3333, 0.3333, 0.3333]

# 7. New: CellForest â€“ smooth approximation of complex 1D function
cf = CellForest(cellscount=64, learning=0.12)
for _ in range(40000):
    x = random.random()
    y = math.sin(x * 10) / 2 + 0.5  # fast oscillating wave
    cf.process(x, target=y, top_k=2)
print("CellForest sin-approx at x=0.1 â†’", cf.process(0.1, train=False, top_k=2)) # expected: 0.9207354924039483
print("CellForest sin-approx at x=0.9 â†’", cf.process(0.9, train=False, top_k=2)) # expected: 0.7060592426208783

# 8. MultiCellForest â€“ robust approximation of complex 2D function (simplified for better convergence)
mcf = MultiCellForest(cellscount=32, icount=2, ocount=1, learning=0.08)
for i in range(50000):
    x = random.random()
    y = random.random()
    target = (math.sin(x * 5) + math.cos(y * 5)) / 4 + 0.5  # simpler 2D oscillation (lower frequency, easier to learn)
    mcf.process([x, y], target=[target], top_k=2)
print("MultiCellForest at [0.1, 0.2] â†’", mcf.process([0.1, 0.2], train=False, top_k=2)[0]) # expected: 0.7549319611180857
print("MultiCellForest at [0.9, 0.8] â†’", mcf.process([0.9, 0.8], train=False, top_k=2)[0]) # expected: 0.09220656536782279
print("MultiCellForest at [0.5, 0.5] â†’", mcf.process([0.5, 0.5], train=False, top_k=2)[0]) # expected: 0.4493321321392557
print("MultiCellForest at [0.0, 1.0] â†’", mcf.process([0.0, 1.0], train=False, top_k=2)[0]) # expected: 0.5709155463658065

# 9. CellNetwork â€“ Learning a Continuous Sine Wave Mapping
# Architecture: 1 input (x) -> 16 hidden neurons -> 1 output (sin(x))
# We use more hidden neurons to capture the curvature of the wave.
net_sine = CellNetwork(layers=[1, 16, 1], learning=0.05)

print("Training CellNetwork on Sine Wave...")
for i in range(150000):
    # Training range: 0 to 2*PI, normalized to 0-1 for stability
    x_val = random.random() 
    # Target: sin(x) mapped from [-1, 1] to [0, 1] range for neural stability
    target_sin = (math.sin(x_val * math.pi * 2) + 1.0) / 2.0
    net_sine.process([x_val], target=[target_sin])

# --- Comprehensive Sine Wave Inference & Accuracy Report ---
test_points = [
    (0.00, "0Â°"),      # Expected: 0
    (0.125, "45Â°"),    # Expected: 0.707
    (0.25, "90Â°"),     # Expected: 1
    (0.50, "180Â°"),    # Expected: 0
    (0.75, "270Â°"),    # Expected: -1
    (1.00, "360Â°")     # Expected: 0
]

print("--- Zevihanthosa CellNetwork: Sine Wave Accuracy Report ---")
total_acc = 0
for x_norm, label in test_points:
    # Get raw prediction [0, 1]
    raw_out = net_sine.process([x_norm], train=False)[0]
    # Map back to [-1, 1]
    predicted_val = (raw_out * 2.0) - 1.0
    # Calculate real value
    actual_val = math.sin(x_norm * math.pi * 2)
    # Calculate error and accuracy percentage
    error = abs(actual_val - predicted_val)
    # Accuracy: 100% minus the error percentage (relative to range 2.0)
    accuracy = max(0, (1.0 - (error / 2.0)) * 100)
    total_acc += accuracy
    
    print(f"Angle: {label:^5} | Pred: {predicted_val:7.4f} | Real: {actual_val:7.4f} | Error: {error:.6f} | Acc: {accuracy:.2f}%")

print(f"Overall CellNetwork Model Consistency: {(total_acc / len(test_points)):.2f}%")

# 10. CellNetworkForest â€“ Ensemble of deep networks for robust 2D function approximation
# (new in 2.7 â€“ stabilized deep forest architecture, handling multi-dimensional inputs)
# Task: Learn sin(2Ï€x) * cos(2Ï€y) + noise (harder, non-linear 2D interaction like modulated waves)
cnf = CellNetworkForest(
    networkcount=24, 
    layers=[2, 12, 1],  # 2 inputs (x,y) â†’ 12 hidden â†’ 1 output
    learning=0.04,
)

print("Training CellNetworkForest (CNF) on Noisy 2D Modulated Wave...")
# Training data: complex 2D surface with gaussian noise for challenge
for i in range(50000):
    x_val = random.random()
    y_val = random.random()
    # Target: sin(2Ï€x) * cos(2Ï€y) â€“ non-separable, oscillatory 2D function
    raw_target = math.sin(x_val * math.pi * 2) * math.cos(y_val * math.pi * 2)
    # Add noise to simulate real-world data imperfection
    noisy_raw = raw_target + random.gauss(0, 0.1)
    # Normalize to [0,1] for framework stability
    target_val = max(0.0, min(1.0, (noisy_raw + 1.0) / 2.0))
    cnf.process([x_val, y_val], target=[target_val], top_k=2)

# --- Comprehensive 2D Modulated Wave Inference & Accuracy Report (comparison) ---
# Test points chosen for diverse regions: peaks, troughs, and interactions
test_points = [
    ([0.00, 0.00], "(0,0)"),     # Expected raw: sin(0)*cos(0) = 0*1 = 0 â†’ norm ~0.5
    ([0.25, 0.00], "(90Â°,0Â°)"),  # Expected: 1*1 = 1 â†’ norm 1.0
    ([0.50, 0.25], "(180Â°,90Â°)"),# Expected: 0*0 = 0 â†’ norm 0.5
    ([0.75, 0.50], "(270Â°,180Â°)"),# Expected: (-1)*(-1) = 1 â†’ norm 1.0
    ([0.00, 0.75], "(0Â°,270Â°)"), # Expected: 0*0 = 0 â†’ norm 0.5
    ([1.00, 1.00], "(360Â°,360Â°)"),# Expected: 0*1 = 0 â†’ norm 0.5
]

print("--- Zevihanthosa CellNetworkForest (CNF â€“ localized ensemble network) Accuracy ---")
total_acc_cnf = 0
for inputs, label in test_points:
    raw_out = cnf.process(inputs, train=False, top_k=2)[0]
    predicted_raw = (raw_out * 2.0) - 1.0  # Denormalize
    actual_raw = math.sin(inputs[0] * math.pi * 2) * math.cos(inputs[1] * math.pi * 2)
    error = abs(actual_raw - predicted_raw)
    accuracy = max(0, (1.0 - (error / 2.0)) * 100)
    total_acc_cnf += accuracy
    print(f"Point: {label:^10} | Pred: {predicted_raw:7.4f} | Real: {actual_raw:7.4f} | Error: {error:.6f} | Acc: {accuracy:.2f}%")

print(f"Overall CNF (Network Forest) Consistency: {total_acc_cnf / len(test_points):.2f}%")
```

### Example Output

```
Cell at 0.6 â†’ 0.032449743937126636
Cell at 0.8 â†’ 0.964736878960268
Current smooth estimate: 0.5
MultiInputCell approx 1/(7+2) from [1/7, 1/2]: 9.154347961916285
x = 0.0 â†’ (xÂ², âˆšx): [0.012386497402471207, 0.2281075329679371]
x = 0.25 â†’ (xÂ², âˆšx): [0.057940683286017716, 0.4640935052228233]
x = 0.5 â†’ (xÂ², âˆšx): [0.23172214712064113, 0.7173350612656512]
x = 1.0 â†’ (xÂ², âˆšx): [0.8788366685322571, 0.9561269185033562]
Discovered function: x**2**-1
Test on x=3 â†’ 1.7320508075688772
Input [0.0, 0.0] â†’ [0.08764712160414166, 0.08764712160414166, 0.08764712160414166]
Input [0.3, 0.3] â†’ [0.19918553864873334, 0.19918553864873334, 0.19918553864873334]
Input [0.5, 0.6] â†’ [0.35441220242318905, 0.35441220242318905, 0.35441220242318905]
Input [1.0, 1.0] â†’ [0.6960045996154952, 0.6960045996154952, 0.6960045996154952]
Input [0.2, 0.8] â†’ [0.31786344456514853, 0.31786344456514853, 0.31786344456514853]
CellForest sin-approx at x=0.1 â†’ 0.9119575184998867
CellForest sin-approx at x=0.9 â†’ 0.6914212261446739
MultiCellForest at [0.1, 0.2] â†’ 0.7611578047035364
MultiCellForest at [0.9, 0.8] â†’ 0.10671033969829927
MultiCellForest at [0.5, 0.5] â†’ 0.387072592260856
MultiCellForest at [0.0, 1.0] â†’ 0.5733994449097569
Training CellNetwork on Sine Wave...
--- Zevihanthosa CellNetwork: Sine Wave Accuracy Report ---
Angle:  0Â°   | Pred:  0.0927 | Real:  0.0000 | Error: 0.092746 | Acc: 95.36%
Angle:  45Â°  | Pred:  0.7472 | Real:  0.7071 | Error: 0.040120 | Acc: 97.99%
Angle:  90Â°  | Pred:  0.9317 | Real:  1.0000 | Error: 0.068326 | Acc: 96.58%
Angle: 180Â°  | Pred:  0.0076 | Real:  0.0000 | Error: 0.007556 | Acc: 99.62%
Angle: 270Â°  | Pred: -0.9310 | Real: -1.0000 | Error: 0.068991 | Acc: 96.55%
Angle: 360Â°  | Pred: -0.1008 | Real: -0.0000 | Error: 0.100796 | Acc: 94.96%
Overall CellNetwork Model Consistency: 96.85%
Training CellNetworkForest (CNF) on Noisy 2D Modulated Wave...
--- Zevihanthosa CellNetworkForest (CNF â€“ localized ensemble network) Accuracy ---
Point:   (0,0)    | Pred:  0.0944 | Real:  0.0000 | Error: 0.094397 | Acc: 95.28%
Point:  (90Â°,0Â°)  | Pred:  0.9105 | Real:  1.0000 | Error: 0.089513 | Acc: 95.52%
Point: (180Â°,90Â°) | Pred:  0.0444 | Real:  0.0000 | Error: 0.044420 | Acc: 97.78%
Point: (270Â°,180Â°) | Pred:  0.9027 | Real:  1.0000 | Error: 0.097274 | Acc: 95.14%
Point: (0Â°,270Â°)  | Pred: -0.0365 | Real: -0.0000 | Error: 0.036544 | Acc: 98.17%
Point: (360Â°,360Â°) | Pred: -0.0783 | Real: -0.0000 | Error: 0.078342 | Acc: 96.08%
Overall CNF (Network Forest) Consistency: 96.33%
```

## Why Zevihanthosa?
* **True Hybrid Intelligence**: Neural + Linear + Memory + Symbolic + Deep Scaled Localized Ensembles + Deep Networks + Multi-Layer Networks.
* **Maximum Transparency, Persistence & Interpretability**.
* **Ultra Lightweight & Fully Portable** (Zero external binary dependencies).
* **Ideal for Education, Research, Embedded AI, and Extreme Specialization Experiments**.
* **Extremely Modular** â€“ easy to extend with new cell types.

### License
MIT License â€” free for any use, commercial or personal.

---
**Zevihanthosa â€” Simplicity meets persistent, interpretable, hybrid intelligence.**

*ZevihaNut/2.9 â€” January 2026: Expert scaling, input compression, anchor blending, quantization and hyper-specialized units. expert form of specialization. Ready.*
